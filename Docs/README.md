
# PySpark Data Processing and Automation Project for Reliance Mart Data


#### Project Overview
This project is a comprehensive data processing pipeline built using PySpark and AWS S3. It involves reading data from S3, processing it locally, and then uploading the processed data back to S3. The main goals are to handle data efficiently, manage errors and edge cases, and ensure data integrity throughout the ETL (Extract, Transform, Load) process.

#### Table of Contents
1. [Project Structure](#project-structure)
2. [Setup Instructions](#setup-instructions)
3. [How to Run the Project](#how-to-run-the-project)
4. [Key Functionalities](#key-functionalities)
5. [Configuration Details](#configuration-details)
6. [Logging and Error Handling](#logging-and-error-handling)
7. [Data Validation and Schema Enforcement](#data-validation-and-schema-enforcement)
8. [Data Mart Creation](#data-mart-creation)
9. [Moving Processed Data to S3](#moving-processed-data-to-s3)
10. [Conclusion](#conclusion)

#### Project Structure

```
project/
│
├── main.py                              # Main script to run the data pipeline
├── resources/
│   └── dev/
│       └── config.py                    # Configuration file with AWS and other settings
├── src/
│   ├── main/
│   │   ├── DataDelete/
│   │   │   └── local_file_delete.py     # Script to delete local files
│   │   ├── DataMoveWithinS3/
│   │   │   └── move_files.py            # Script to move files within S3
│   │   ├── DataRead/
│   │   │   ├── DataBaseRead.py          # Script to read data from a database
│   │   │   └── s3_fileRead.py           # Script to read files from S3
│   │   ├── DataUpload/
│   │   │   └── upload_to_s3.py          # Script to upload files to S3
│   │   ├── DataWrite/
│   │   │   └── DataFrameFormat_writer.py # Script to write DataFrames in various formats
│   │   ├── transformations/
│   │   │   └── jobs/
│   │   │       ├── customer_mart_sql_transform_write.py # SQL-based transformation for customer data mart
│   │   │       ├── dimensions_tables_join.py           # Script to join dimension tables
│   │   │       └── sales_mart_sql_transform_write.py   # SQL-based transformation for sales data mart
│   │   └── utility/
│   │       ├── encrypt_decrypt.py        # Utility functions for encryption and decryption
│   │       ├── s3_client_object.py       # S3 client provider
│   │       ├── logging_config.py         # Configuration for logging
│   │       ├── spark_session.py          # Utility to create Spark session
│   │       └── sql_session_script.py     # Utility to manage SQL sessions
│   └── download/
│       └── aws_file_downloader.py        # Script to download files from AWS
└── README.md                             # Documentation file
```

#### Setup Instructions

1. **Environment Setup**:
   - Install the required Python packages by running:
     ```bash
     pip install -r requirements.txt
     ```
   - Ensure that you have PySpark, AWS SDK for Python (boto3), and any other dependencies installed.

2. **AWS Credentials**:
   - AWS credentials should be securely stored in the `config.py` file in an encrypted format. Update the file with your credentials and other configuration details.

3. **Local and S3 Directories**:
   - Update the `config.py` file to point to your local directories for storing data and specify the correct S3 buckets and paths.

#### How to Run the Project

1. **Prepare Your Environment**:
   - Make sure your AWS credentials are configured properly and you have access to the specified S3 buckets.

2. **Run the Main Script**:
   - Execute the `main.py` script to start the data processing pipeline.
     ```bash
     python main.py
     ```

3. **Monitor the Logs**:
   - Check the logs generated by the script to monitor progress and identify any issues.

#### Key Functionalities

- **AWS S3 Client Initialization**:
  The script initializes an AWS S3 client using credentials provided in the configuration file. This client is used for all interactions with S3.

- **Reading Data from S3**:
  The script reads data from the specified S3 bucket and downloads it to a local directory for processing.

- **Local File Processing**:
  Data files downloaded from S3 are validated against a predefined schema. Only files with the correct schema are processed further.

- **Data Transformation**:
  SQL-based transformations are applied to the data to create various data marts such as customer and sales marts.

- **Error Handling**:
  Files that do not meet the schema requirements are moved to an error directory for further inspection.

- **Uploading Processed Data to S3**:
  Once processing is complete, the script uploads the transformed data back to the specified S3 bucket.

#### Configuration Details

- **AWS Configuration**:
  The `config.py` file contains encrypted AWS credentials and configuration settings such as bucket names, directory paths, and database connection details.

- **Local Directories**:
  Specify local directories for temporary storage of downloaded files and processed data.

- **Data Validation**:
  Schema validation settings are defined in the configuration file to ensure that only valid data is processed.

#### Logging and Error Handling

- **Logging**:
  The script uses a custom logging configuration to log messages at various levels (INFO, ERROR, etc.). Logs are crucial for monitoring the pipeline and troubleshooting issues.

- **Error Handling**:
  Comprehensive error handling is in place to manage issues such as missing files, incorrect schemas, and AWS S3 errors.

#### Data Validation and Schema Enforcement

- **Schema Validation**:
  Before processing, each CSV file is checked against a mandatory schema defined in the configuration. Files with missing columns are moved to an error directory.

- **Additional Column Handling**:
  If a file contains extra columns not specified in the schema, these are concatenated into an "additional_column" field.

#### Data Mart Creation

- **Customer Data Mart**:
  A data mart is created by selecting specific fields from the processed data and storing it in a Parquet file locally and on S3.

- **Sales Data Mart**:
  A similar process is followed for the sales data mart, ensuring that all relevant data is captured for reporting purposes.

#### Moving Processed Data to S3

- **S3 Upload**:
  The processed data is uploaded back to S3 in the specified directory for further analysis and reporting.

- **S3 Error Handling**:
  The script checks for any issues during the upload process and logs them accordingly.

#### Conclusion

This project demonstrates the use of PySpark and AWS S3 to create a robust data processing pipeline. It covers the full ETL process, from reading data from S3, processing it locally with PySpark, validating and transforming the data, and finally uploading the processed data back to S3. Proper logging and error handling ensure the reliability and maintainability of the pipeline.